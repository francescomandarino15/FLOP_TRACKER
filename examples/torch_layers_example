import os
import sys

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# Rende importabile flops_tracker.py dalla cartella superiore (FLOPs_TRACKER)
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from flops_tracker import FlopsTracker


class ComplexNet(nn.Module):
    """
    Modello PyTorch "dimostrativo" che include molti layer diversi:
    - Padding (ZeroPad2d)
    - Convoluzioni (Conv2d)
    - Pooling (MaxPool2d, AdaptiveAvgPool2d)
    - Normalizzazione (BatchNorm2d, LayerNorm)
    - Dropout (Dropout2d, Dropout)
    - Vision / pixel shuffle (PixelShuffle)
    - Trasformazioni di shape (Flatten, view/permute)
    - Transformer Encoder layer (Multi-Head Attention + FFN)
    - LSTM ricorrente
    - Fully connected finale (Linear)
    """

    def __init__(self, num_classes: int = 10, img_size: int = 32):
        super().__init__()

        # 1) Sezione "vision" (CNN + padding + pooling + batchnorm + pixel shuffle)
        self.pad = nn.ZeroPad2d(1)  # padding 1 pixel su tutti i lati

        self.conv_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=0),  # dopo ZeroPad2d
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2),  # dimezza H,W
        )

        self.conv_block2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Dropout2d(p=0.1),
        )

        # PixelShuffle richiede canali = r^2 * C_out
        # Qui aumentiamo i canali e poi shuffliamo
        self.pre_shuffle = nn.Conv2d(32, 32 * 4, kernel_size=1)  # 4 = r^2 con r=2
        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=2)   # upsample spaziale

        # Riduciamo di nuovo la dimensione spaziale
        self.pool2 = nn.AdaptiveAvgPool2d((8, 8))

        # 2) Flatten + Linear per ottenere una rappresentazione "sequenza"
        conv_out_dim = 32 * 8 * 8
        self.flatten = nn.Flatten()
        self.fc_to_seq = nn.Linear(conv_out_dim, 128)  # 128 = L * d_model

        # 3) LayerNorm + TransformerEncoderLayer
        self.d_model = 32
        self.seq_len = 4  # 4 * 32 = 128
        self.layernorm = nn.LayerNorm(self.d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=4,
            dim_feedforward=128,
            dropout=0.1,
            batch_first=True,  # (batch, seq, dim)
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)

        # 4) LSTM ricorrente sulla sequenza trasformata
        self.lstm = nn.LSTM(
            input_size=self.d_model,
            hidden_size=64,
            num_layers=1,
            batch_first=True,
            bidirectional=False,
        )

        self.dropout = nn.Dropout(p=0.2)

        # 5) Classificatore finale
        self.classifier = nn.Linear(64, num_classes)

    def forward(self, x):
        # x: (batch, 3, 32, 32)
        # 1) CNN + padding + pooling + pixel shuffle
        x = self.pad(x)                # -> (B, 3, 34, 34)
        x = self.conv_block1(x)        # -> (B, 16, 17, 17) -> pool -> (B, 16, ~8, ~8)
        x = self.conv_block2(x)        # -> (B, 32, 8, 8) circa

        x = self.pre_shuffle(x)        # -> (B, 32*4, 8, 8)
        x = self.pixel_shuffle(x)      # -> (B, 32, 16, 16)
        x = self.pool2(x)              # -> (B, 32, 8, 8)

        # 2) Flatten + Linear -> vettore di dimensione 128
        x = self.flatten(x)            # (B, 32*8*8)
        x = self.fc_to_seq(x)          # (B, 128)

        # 3) Interpretiamo il vettore come sequenza (B, L, d_model)
        x = x.view(-1, self.seq_len, self.d_model)  # (B, 4, 32)
        x = self.layernorm(x)         # (B, 4, 32)
        x = self.transformer_encoder(x)  # (B, 4, 32)

        # 4) LSTM sulla sequenza
        x, (h_n, c_n) = self.lstm(x)  # x: (B, 4, 64), h_n: (1, B, 64)
        # Prendiamo lo stato nascosto finale
        h_last = h_n[-1]              # (B, 64)

        h_last = self.dropout(h_last)

        # 5) Classificatore finale
        logits = self.classifier(h_last)  # (B, num_classes)
        return logits


def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Dataset fittizio: immagini 32x32 RGB, 10 classi
    x = torch.randn(256, 3, 32, 32)
    y = torch.randint(0, 10, (256,))
    ds = TensorDataset(x, y)
    train_loader = DataLoader(ds, batch_size=32, shuffle=True)

    model = ComplexNet(num_classes=10)
    # Supporto opzionale DataParallel (se hai più GPU)
    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        model = nn.DataParallel(model)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.CrossEntropyLoss()

    ft = FlopsTracker(run_name="torch_complex_model").torch_bind(
        model=model,
        optimizer=optimizer,
        loss_fn=loss_fn,
        train_loader=train_loader,
        device=device,
        epochs=2,
        backend="torch",
        log_per_batch=True,
        log_per_epoch=True,
        export_path="torch_complex_model_flops.csv",
        use_wandb=False,
    )

    # opzionali: il tracker stampa già i FLOPs totali
    print("Raw FLOPs:", ft.raw_flops)
    print("Total FLOPs:", ft.total_flops)


if __name__ == "__main__":
    main()
